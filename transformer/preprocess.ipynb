{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef6c012-ebc6-46d3-a582-3fb2ef8ae89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - sentencepiece tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29213899-7242-4e2b-ad39-72e05851d477",
   "metadata": {},
   "source": [
    "## Preprocess WMT14 Dataset\n",
    "- step 1. Download Raw Data(Parquet) and Save it as txt\n",
    "- step 2. Train tokenizer(BPE)\n",
    "- step 3. Preprocess(tokenize) the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940bd0f-076a-4303-bdb5-65d2956add3c",
   "metadata": {},
   "source": [
    "#### 1. Download WMT14-(en, de)\n",
    "- The file format is parquet\n",
    "- Download it first and Save it line by line on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ee852e-9d39-4e17-bfb6-bc0ab9bf451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download WMT14 English-German dataset as parquet\n",
      "Train Samples: 4508785\n",
      "Validation Samples: 3000\n",
      "Test Samples: 3003\n"
     ]
    }
   ],
   "source": [
    "# Download WMT14(english-german) and convert parquet to text\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def save_split(data, save_dir, split_name):\n",
    "    en_file = os.path.join(save_dir, f\"{split_name}.en\")\n",
    "    de_file = os.path.join(save_dir, f\"{split_name}.de\")\n",
    "\n",
    "    with open(en_file, \"w\", encoding=\"utf-8\") as f_en, open(de_file, \"w\", encoding=\"utf-8\") as f_de:\n",
    "        for example in tqdm(data, desc=f\"Saving {split_name}\"):\n",
    "            en_text = example[\"translation\"][\"en\"]\n",
    "            de_text = example[\"translation\"][\"de\"]\n",
    "\n",
    "            f_en.write(en_text + \"\\n\")\n",
    "            f_de.write(de_text + \"\\n\")\n",
    "\n",
    "    print(f\"Saved to {save_dir} - {split_name}.en and {split_name}.de\")\n",
    "\n",
    "print(\"Download WMT14 English-German dataset as parquet\")\n",
    "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
    "\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "print(f\"Train Samples: {len(train_data)}\")\n",
    "print(f\"Validation Samples: {len(val_data)}\")\n",
    "print(f\"Test Samples: {len(test_data)}\")\n",
    "\n",
    "data_dir = \"data\"\n",
    "save_dir = f\"{data_dir}/wmt14\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# save_split(train_data, save_dir, \"train\")\n",
    "# save_split(val_data, save_dir, \"valid\")\n",
    "# save_split(test_data, save_dir, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db883206-fa09-451d-8ea5-6efefcfb8fe8",
   "metadata": {},
   "source": [
    "## 2. Train BPE Tokenizer\n",
    "- CPU core수를 충분히 사용!(속도 차이가 심함)\n",
    "- Source and Target에 따라 2개의 Tokenizer 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6d3110-323c-4d37-b41a-ef3472858398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Enlgish Tokenizer\n",
      "Tokenizer save : data/tokenizers/en_tokenizer.model, data/tokenizers/en_tokenizer.vocab\n",
      "Train German Tokenizer\n",
      "Tokenizer save : data/tokenizers/de_tokenizer.model, data/tokenizers/de_tokenizer.vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainer_interface.cc(382) LOG(WARNING) Found too long line (25081 > 4192).\n",
      "trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (4508727), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(382) LOG(WARNING) Found too long line (4899 > 4192).\n",
      "trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (4508745), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n"
     ]
    }
   ],
   "source": [
    "# Train sentencepiece tokenizer\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "def train_sentencepiece_tokenizer(input_files, model_prefix, vocab_size):\n",
    "    \"\"\"Train BPE tokenizer\"\"\"\n",
    "    input_str = \",\".join(input_files)\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input = input_str,\n",
    "        model_prefix = model_prefix,\n",
    "        vocab_size = vocab_size,\n",
    "        character_coverage = 1.0,\n",
    "        model_type = \"bpe\",\n",
    "        pad_id = 0,\n",
    "        unk_id = 1,\n",
    "        bos_id = 2,\n",
    "        eos_id = 3,\n",
    "        pad_piece = \"<pad>\",\n",
    "        unk_piece = \"<unk>\",\n",
    "        bos_piece = \"<s>\",\n",
    "        eos_piece = \"</s>\",\n",
    "        train_extremely_large_corpus=True,  # 대용량 데이터용\n",
    "        num_threads=os.cpu_count(),  # 모든 CPU 코어 사용\n",
    "        minloglevel=1  # 0: INFO, 1: WARNING, 2: ERROR\n",
    "    )\n",
    "    print(f\"Tokenizer save : {model_prefix}.model, {model_prefix}.vocab\")\n",
    "\n",
    "vocab_size = 32000\n",
    "os.makedirs(f\"{data_dir}/tokenizers\", exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"Train Enlgish Tokenizer\")\n",
    "train_sentencepiece_tokenizer(\n",
    "    input_files=[f\"{save_dir}/train.en\"],\n",
    "    model_prefix=f\"{data_dir}/tokenizers/en_tokenizer\",\n",
    "    vocab_size=vocab_size,\n",
    ")\n",
    "print(\"Train German Tokenizer\")\n",
    "train_sentencepiece_tokenizer(\n",
    "    input_files=[f\"{save_dir}/train.de\"],\n",
    "    model_prefix=f\"{data_dir}/tokenizers/de_tokenizer\",\n",
    "    vocab_size=vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602d436-6bff-4a92-9fa1-9be4b536c38b",
   "metadata": {},
   "source": [
    "## 3. Tokenize source and target data\n",
    "- 학습완료한 en-tokenizer를 이용하여 Source(en) 파일 전처리\n",
    "- 학습완료한 de-tokenizer를 이용하여 Target(de) 파일 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b759491-2262-4ff9-a2f5-434c8a474bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load tokenizers\n",
      "Read and Tokenize and Save\n",
      "Source File: data/wmt14/train.en\n",
      "Target File: data/wmt14/train.de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34bf72d15fe4f90bac9beb68b8d32b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total: 4509342\n",
      "Filtered: 37872\n",
      "Kept: 4471470\n",
      "\n",
      "\n",
      "\n",
      "Load tokenizers\n",
      "Read and Tokenize and Save\n",
      "Source File: data/wmt14/valid.en\n",
      "Target File: data/wmt14/valid.de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52732ebac67043c69379fe727cc6b149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total: 3000\n",
      "Filtered: 7\n",
      "Kept: 2993\n",
      "\n",
      "\n",
      "\n",
      "Load tokenizers\n",
      "Read and Tokenize and Save\n",
      "Source File: data/wmt14/test.en\n",
      "Target File: data/wmt14/test.de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf10b97c7d498988fd4e3e99179ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total: 3003\n",
      "Filtered: 1\n",
      "Kept: 3002\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data with tokenizer\n",
    "def tokenize_data(\n",
    "    src_file,\n",
    "    tgt_file,\n",
    "    src_tokenizer,\n",
    "    tgt_tokenizer,\n",
    "    output_prefix,\n",
    "    max_len = 100,\n",
    "):\n",
    "    print(\"Load tokenizers\")\n",
    "    sp_src = spm.SentencePieceProcessor()\n",
    "    sp_tgt = spm.SentencePieceProcessor()\n",
    "    sp_src.load(src_tokenizer)\n",
    "    sp_tgt.load(tgt_tokenizer)\n",
    "    \n",
    "    print(\"Read and Tokenize and Save\")\n",
    "    print(f\"Source File: {src_file}\")\n",
    "    print(f\"Target File: {tgt_file}\")\n",
    "    with open(src_file, \"r\", encoding=\"utf-8\") as f_src, \\\n",
    "        open(tgt_file, \"r\", encoding=\"utf-8\") as f_tgt, \\\n",
    "        open(f\"{output_prefix}.src\", \"w\", encoding=\"utf-8\") as f_out_src, \\\n",
    "        open(f\"{output_prefix}.tgt\", \"w\", encoding=\"utf-8\") as f_out_tgt:\n",
    "            total = 0\n",
    "            filtered = 0 # over max_len\n",
    "    \n",
    "            for src_line, tgt_line in zip(f_src, f_tgt):\n",
    "                total += 1\n",
    "    \n",
    "                # Tokenize\n",
    "                src_tokens = sp_src.encode_as_pieces(src_line.strip())\n",
    "                tgt_tokens = sp_tgt.encode_as_pieces(tgt_line.strip())\n",
    "    \n",
    "                if len(src_tokens) <= max_len and len(tgt_tokens) <= max_len:\n",
    "                    f_out_src.write(\" \".join(src_tokens) + \"\\n\")\n",
    "                    f_out_tgt.write(\" \".join(tgt_tokens) + \"\\n\")\n",
    "                else:\n",
    "                    filtered += 1\n",
    "                    \n",
    "    print(f\"\\n\\nTotal: {total}\")\n",
    "    print(f\"Filtered: {filtered}\")\n",
    "    print(f\"Kept: {total - filtered}\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "os.makedirs(f\"{data_dir}/processed\", exist_ok=True)\n",
    "\n",
    "for split_name in [\"train\", \"valid\", \"test\"]:\n",
    "    tokenize_data(\n",
    "        src_file=f\"{data_dir}/wmt14/{split_name}.en\",\n",
    "        tgt_file=f\"{data_dir}/wmt14/{split_name}.de\",\n",
    "        src_tokenizer=f\"{data_dir}/tokenizers/en_tokenizer.model\",\n",
    "        tgt_tokenizer=f\"{data_dir}/tokenizers/de_tokenizer.model\",\n",
    "        output_prefix=f\"{data_dir}/processed/{split_name}\",\n",
    "        max_len=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d2b56-65b8-41e6-b4fc-788be2960573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "261feb04-3d6f-4e80-93d6-b727f14082ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:\n",
      "▁Res umption ▁of ▁the ▁session\n",
      "\n",
      "Encoded Tokens:\n",
      "[1965, 9905, 1180, 25, 9, 4826]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁I ▁declare ▁resumed ▁the ▁session ▁of ▁the ▁European ▁Parliament ▁adjourned ▁on ▁Friday ▁17 ▁December ▁1999, ▁and ▁I ▁would ▁like ▁once ▁again ▁to ▁wish ▁you ▁a ▁happy ▁new ▁year ▁in ▁the ▁hope ▁that ▁you ▁enjoyed ▁a ▁pleasant ▁festive ▁period .\n",
      "\n",
      "Encoded Tokens:\n",
      "[57, 8816, 11284, 9, 4826, 25, 9, 218, 466, 18978, 64, 6453, 2263, 2973, 9778, 34, 57, 295, 382, 1646, 548, 32, 1509, 106, 5, 3395, 342, 393, 28, 9, 1232, 65, 106, 6918, 5, 4290, 22668, 1495, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁Although , ▁as ▁you ▁will ▁have ▁seen , ▁the ▁dread ed ▁' mill ennium ▁bug ' ▁failed ▁to ▁material ise , ▁still ▁the ▁people ▁in ▁a ▁number ▁of ▁countries ▁suffered ▁a ▁series ▁of ▁natural ▁disasters ▁that ▁truly ▁were ▁dreadful .\n",
      "\n",
      "Encoded Tokens:\n",
      "[3275, 536, 103, 106, 164, 146, 2312, 536, 9, 16376, 1111, 736, 4166, 173, 28810, 1198, 8436, 736, 4765, 32, 1579, 53, 28805, 536, 907, 9, 474, 28, 5, 891, 25, 516, 7016, 5, 2728, 25, 1969, 7213, 65, 4514, 490, 19962, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁You ▁have ▁requested ▁a ▁debate ▁on ▁this ▁subject ▁in ▁the ▁course ▁of ▁the ▁next ▁few ▁days , ▁during ▁this ▁part - session .\n",
      "\n",
      "Encoded Tokens:\n",
      "[729, 146, 5254, 5, 1073, 64, 119, 1368, 28, 9, 961, 25, 9, 1158, 1108, 1692, 536, 1042, 119, 246, 247, 4826, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁In ▁the ▁meantime , ▁I ▁should ▁like ▁to ▁observe ▁a ▁minute ' ▁s ▁silence , ▁as ▁a ▁number ▁of ▁Members ▁have ▁requested , ▁on ▁behalf ▁of ▁all ▁the ▁victims ▁concerned , ▁particularly ▁those ▁of ▁the ▁terrible ▁storms , ▁in ▁the ▁various ▁countries ▁of ▁the ▁European ▁Union .\n",
      "\n",
      "Encoded Tokens:\n",
      "[253, 9, 9699, 536, 57, 333, 382, 32, 8315, 5, 5199, 736, 15, 9498, 536, 103, 5, 891, 25, 1717, 146, 5254, 536, 64, 1733, 25, 189, 9, 3863, 1886, 536, 1363, 647, 25, 9, 8620, 22968, 536, 28, 9, 1428, 516, 25, 9, 218, 443, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁Please ▁rise , ▁then , ▁for ▁this ▁minute ' ▁s ▁silence .\n",
      "\n",
      "Encoded Tokens:\n",
      "[2152, 3865, 536, 805, 536, 62, 119, 5199, 736, 15, 9498, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁( The ▁House ▁rose ▁and ▁observed ▁a ▁minute ' ▁s ▁silence )\n",
      "\n",
      "Encoded Tokens:\n",
      "[183, 92, 1376, 10723, 34, 7926, 5, 5199, 736, 15, 9498, 2953]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁Madam ▁President , ▁on ▁a ▁point ▁of ▁order .\n",
      "\n",
      "Encoded Tokens:\n",
      "[1666, 394, 536, 64, 5, 633, 25, 724, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁You ▁will ▁be ▁aware ▁from ▁the ▁press ▁and ▁television ▁that ▁there ▁have ▁been ▁a ▁number ▁of ▁bomb ▁explos ions ▁and ▁killings ▁in ▁Sri ▁Lanka .\n",
      "\n",
      "Encoded Tokens:\n",
      "[729, 164, 55, 2041, 175, 9, 1529, 34, 4565, 65, 281, 146, 311, 5, 891, 25, 6928, 9616, 28804, 184, 34, 22434, 28, 13124, 16560, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁One ▁of ▁the ▁people ▁assass inated ▁very ▁recently ▁in ▁Sri ▁Lanka ▁was ▁Mr ▁K um ar ▁Pon nam bal am , ▁who ▁had ▁visited ▁the ▁European ▁Parliament ▁just ▁a ▁few ▁months ▁ago .\n",
      "\n",
      "Encoded Tokens:\n",
      "[1942, 25, 9, 474, 13794, 28, 233, 379, 2684, 28, 13124, 16560, 217, 284, 397, 9905, 402, 19657, 48, 117, 1680, 322, 536, 344, 654, 6267, 9, 218, 466, 556, 5, 1108, 2059, 2162, 514]\n",
      "\n",
      "\n",
      "\n",
      "Token:\n",
      "▁Would ▁it ▁be ▁appropriate ▁for ▁you , ▁Madam ▁President , ▁to ▁write ▁a ▁letter ▁to ▁the ▁Sri ▁L ank an ▁President ▁expressing ▁Parliament ' s ▁regret ▁at ▁his ▁and ▁the ▁other ▁violent ▁deaths ▁in ▁Sri ▁Lanka ▁and ▁urging ▁her ▁to ▁do ▁everything ▁she ▁possibly ▁can ▁to ▁seek ▁a ▁peaceful ▁reconciliation ▁to ▁a ▁very ▁difficult ▁situation ?\n",
      "\n",
      "Encoded Tokens:\n",
      "[7764, 100, 55, 2333, 62, 106, 536, 1666, 394, 536, 32, 5185, 5, 5184, 32, 9, 13124, 201, 111, 28828, 111, 394, 9763, 466, 736, 15, 4061, 149, 534, 34, 9, 335, 9339, 9160, 28, 13124, 16560, 34, 18272, 1084, 32, 258, 2292, 1413, 6482, 182, 32, 2760, 5, 4329, 10887, 32, 5, 379, 1377, 1078, 6291]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 예시\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load(\"data/tokenizers/en_tokenizer.model\")\n",
    "with open(f\"data/processed/train.src\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, tokenized_line in enumerate(f):\n",
    "        print(f\"Token:\\n{tokenized_line}\")\n",
    "        print(f\"Encoded Tokens:\\n{tokenizer.encode_as_ids(tokenized_line)}\", )\n",
    "        print(\"\\n\\n\")\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a98be7-d62b-4977-8eab-7156450c435b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905de27-20bc-41f1-af69-ee5aa3218e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f2e6e-47fb-4d63-af81-c1b7c41f868a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
